# Learning Experience

- Biggest difficulties: Getting the wrong attribute for MLPGates & setting the scaled attention product to the full hidden dim (2048) instead of per head dimension (32)
- Biggest surprise: how the LLMs actually read the raw inputs with special tokens. Because it's so boring I suppose because I already heard about how temperature works and the attention mechanism.
