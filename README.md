# Learning Experience

- Biggest difficulties: Getting the wrong attribute for MLPGates & setting the scaled attention product to the full hidden dim (2048) instead of per head dimension (32) & installing candle-fast-attention which uses parallel build process for all its CUDA kernel filling up my swap and freezing my laptop (until I realized what's happening)
- Biggest surprise: how the LLMs actually read the raw inputs with special tokens. Because it's so boring I suppose because I already heard about how temperature works and the attention mechanism.
